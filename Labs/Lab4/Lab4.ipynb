{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Lab 4\"\n",
    "author: \"Chloe Feehan\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "    embed-resources: true\n",
    "    toc: true\n",
    "theme: flatly\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffee Lovers Unite!\n",
    "If caffeine is one of the most popular drugs, then coffee is likely one of the most popular delivery systems for it. Aside from caffeine, people enjoy the wonderful variety of coffee-related drinks. Let’s do a rough investigation of the “market share” by two of the top coffee chains in the United States!\n",
    "\n",
    "World Population Review provides some great data on store locations and chain prevalence. Check out this page for the Starbucks Coffee locations in the United States. Notice that this page only really gives the name of the state and the number of locations in that state.\n",
    "\n",
    "# Scrape the Location Counts\n",
    "1. Use the beautifulsoup library to scrape the data (from the link above) on state names and corresponding number of store locations, for the following chains:\n",
    "\n",
    "Starbucks\n",
    "\n",
    "Dunkin’ Donuts\n",
    "\n",
    "2. Parse, merge and tidy your data. Think carefully about what the tidy version of this dataset is with multiple years represented on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"https://worldpopulationreview.com/state-rankings/starbucks-stores-by-state\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", class_ = \"wpr-table\")\n",
    "\n",
    "# scrape starbucks data\n",
    "\n",
    "rows = []\n",
    "states_list = []\n",
    "\n",
    "# get state names and add to states list\n",
    "for th in table.find_all(\"th\")[4:]:\n",
    "    states = th.get_text(strip = True)\n",
    "    states_list.append(states)\n",
    "\n",
    "for i, tr in enumerate(table.find_all(\"tr\")[1:]):\n",
    "    # iterate across states list to get corresponding state for each row\n",
    "    state = states_list[i]\n",
    "\n",
    "    # get all the rows in the dataset\n",
    "    cells = tr.find_all(\"td\")\n",
    "\n",
    "    # get all 2023 stores\n",
    "    stores2023_tag = cells[0].find(\"a\") or cells[0]\n",
    "    stores2023 = stores2023_tag.get_text(strip = True)\n",
    "    \n",
    "    #get all 2021 stores\n",
    "    stores2021_tag = cells[1].find(\"a\") or cells[1]\n",
    "    stores2021 = stores2021_tag.get_text(strip = True)\n",
    "\n",
    "    # get all 2024 stores\n",
    "    stores2024_tag = cells[2].find(\"a\") or cells[2]\n",
    "    stores2024 = stores2024_tag.get_text(strip = True)\n",
    "\n",
    "    # add columns to a list\n",
    "    rows.append({\n",
    "        \"state\" : state,\n",
    "        \"2023\" : stores2023,\n",
    "        \"2021\" : stores2021,\n",
    "        \"2024\" : stores2024\n",
    "\n",
    "    })\n",
    "\n",
    "# convert to a dataframe\n",
    "starbucks = pd.DataFrame(rows)\n",
    "\n",
    "# clean and tidy data\n",
    "starbucks_clean = starbucks.melt(id_vars = [\"state\"], var_name = \"year\", value_name = \"store_count\" )\n",
    "starbucks_clean[\"location\"] = \"Starbucks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Dunkin Donuts\n",
    "\n",
    "response = requests.get(\"https://worldpopulationreview.com/state-rankings/dunkin-donuts-by-state\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", class_ = \"wpr-table\")\n",
    "\n",
    "# scrape dunkin data\n",
    "\n",
    "rows = []\n",
    "states_list = []\n",
    "\n",
    "# get state names and add to states list\n",
    "for th in table.find_all(\"th\")[3:]:\n",
    "    states = th.get_text(strip = True)\n",
    "    states_list.append(states)\n",
    "\n",
    "for i, tr in enumerate(table.find_all(\"tr\")[1:]):\n",
    "    # iterate across states list to get corresponding state for each row\n",
    "    state = states_list[i]\n",
    "\n",
    "    # get all the rows in the dataset\n",
    "    cells = tr.find_all(\"td\")\n",
    "\n",
    "    # get all 2024 stores\n",
    "    stores2024_tag = cells[0].find(\"a\") or cells[0]\n",
    "    stores2024 = stores2024_tag.get_text(strip = True)\n",
    "\n",
    "    # get all 2023 stores\n",
    "    stores2023_tag = cells[1].find(\"a\") or cells[1]\n",
    "    stores2023 = stores2023_tag.get_text(strip = True)\n",
    "\n",
    "    # add columns to a list\n",
    "    rows.append({\n",
    "        \"state\" : state,\n",
    "        \"2023\" : stores2023,\n",
    "        \"2024\" : stores2024\n",
    "\n",
    "    })\n",
    "\n",
    "# convert to a dataframe\n",
    "dunkin = pd.DataFrame(rows)\n",
    "\n",
    "# clean and tidy data\n",
    "dunkin_clean = dunkin.melt(id_vars = [\"state\"], var_name = \"year\", value_name = \"store_count\")\n",
    "dunkin_clean[\"location\"] = \"Dunkin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# merge dunkin and starbucks datasets\n",
    "\n",
    "merged_coffee = starbucks_clean.merge(dunkin_clean, on = [\"state\", \"year\", \"location\", \"store_count\"], \n",
    "how = \"outer\")\n",
    "\n",
    "merged_coffee[\"year\"] = merged_coffee[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplemental Data\n",
    "4. Scrape the state names and populations from this wikipedia page. Merge these data with your coffee dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = requests.get(\"https://simple.wikipedia.org/wiki/List_of_U.S._states_by_population\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "tables = soup.find_all(\"table\", class_ = \"wikitable\")\n",
    "\n",
    "#extract table\n",
    "table = tables[0]\n",
    "\n",
    "# scrape population data\n",
    "rows = []\n",
    "\n",
    "for tr in table.find_all(\"tr\")[1:]:\n",
    "    cells = tr.find_all(\"td\")\n",
    "\n",
    "    # get state values\n",
    "    state_tag = cells[2].find(\"a\") or cells[2]\n",
    "    state = state_tag.get_text(strip = True)\n",
    "\n",
    "    # get population values\n",
    "    pop_tag = cells[3].find(\"a\") or cells[3]\n",
    "    population = pop_tag.get_text(strip = True)\n",
    "\n",
    "    # append data\n",
    "    rows.append({\n",
    "        \"state\" : state,\n",
    "        \"population\" : population\n",
    "    })\n",
    "\n",
    "# convert to a df\n",
    "population = pd.DataFrame(rows)\n",
    "\n",
    "# merge dataframes\n",
    "merged_pop = merged_coffee.merge(population, on = \"state\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. Find the revenue, stock price, or your financial metric of choice for each of the companies listed above (if you can find a website to scrape these from that’s great!…but it’s okay if you manually enter these). Merge these values into your big dataset. Note: these values may be repeated for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# scrape starbucks revenue table from website\n",
    "response = requests.get(\"https://companiesmarketcap.com/starbucks/revenue/\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for tr in table.find_all(\"tr\")[1:]:\n",
    "    cells = tr.find_all(\"td\")\n",
    "    \n",
    "    # get year values \n",
    "    year_tag = cells[0].find(\"a\") or cells[0]\n",
    "    year = year_tag.get_text(strip = True)\n",
    "\n",
    "    # get revenue values\n",
    "    revenue_tag = cells[1].find(\"a\") or cells[1]\n",
    "    revenue = revenue_tag.get_text(strip = True)\n",
    "\n",
    "    # append rows\n",
    "    rows.append({\n",
    "        \"year\" : year,\n",
    "        \"revenue\" : revenue\n",
    "    })\n",
    "\n",
    "# convert to a df\n",
    "starbucks_revenue = pd.DataFrame(rows)\n",
    "\n",
    "# remove unnecessary items in the columns\n",
    "starbucks_revenue[\"year\"] = starbucks_revenue[\"year\"].str.replace(\"2024(TTM)\", \"2024\").astype(int)\n",
    "starbucks_revenue[\"revenue\"] = starbucks_revenue[\"revenue\"].str.replace(\"$\",\"\")\n",
    "starbucks_revenue[\"revenue\"] = starbucks_revenue[\"revenue\"].str.replace(\" B\",\"\").astype(float)\n",
    "\n",
    "# add location column\n",
    "starbucks_revenue[\"location\"] = \"Starbucks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# create Dunkin Donuts df - *** CHECK 2024 REVENUE FOR DUNKIN\n",
    "dunkin_revenue = pd.DataFrame({\n",
    "    \"year\" :[2023, 2024],\n",
    "    \"revenue\" : [1.4, 1.6],\n",
    "    \"location\" : [\"Dunkin\"] * 2\n",
    "})\n",
    "\n",
    "# merge revenue data\n",
    "merged_revenue = pd.concat([starbucks_revenue, dunkin_revenue], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# merge revenue data with merged coffee data\n",
    "merged_rev_coffee = merged_coffee.merge(merged_revenue, on = [\"location\", \"year\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create a region variable in your dataset according to the scheme on this wikipedia page: Northeast, Midwest, South, West. You do not need to scrape this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# create region variable\n",
    "regions_dict = {\n",
    "    \"New England\": [\"Connecticut\", \"Maine\", \"Massachusetts\", \"New Hampshire\", \"Rhode Island\", \"Vermont\"],\n",
    "    \"Mideast\": [\"Delaware\", \"Maryland\", \"New Jersey\", \"New York\", \"Pennsylvania\", \"Washington, D.C.\"],\n",
    "    \"Great Lakes\": [\"Illinois\", \"Indiana\", \"Michigan\", \"Ohio\", \"Wisconsin\"],\n",
    "    \"Plains\": [\"Iowa\", \"Kansas\", \"Minnesota\", \"Missouri\", \"Nebraska\", \"North Dakota\", \"South Dakota\"],\n",
    "    \"Southeast\": [\"Alabama\", \"Arkansas\", \"Florida\", \"Georgia\", \"Kentucky\", \"Louisiana\", \"Mississippi\", \"North Carolina\", \"South Carolina\", \"Tennessee\", \"Virginia\", \"West Virginia\"],\n",
    "    \"Southwest\": [\"Arizona\", \"New Mexico\", \"Oklahoma\", \"Texas\"],\n",
    "    \"Rocky Mountain\": [\"Colorado\", \"Idaho\", \"Montana\", \"Utah\", \"Wyoming\"],\n",
    "    \"Far West\": [\"Alaska\", \"California\", \"Hawaii\", \"Nevada\", \"Oregon\", \"Washington\"],\n",
    "}\n",
    "\n",
    "# create regions df\n",
    "regions = pd.DataFrame([(state, region) for region, states in regions_dict.items() for state in states], \n",
    "                            columns=['state', 'Region'])\n",
    "\n",
    "# merge df with dataset\n",
    "all_data = merged_rev_coffee.merge(regions, on = \"state\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze\n",
    "7. Assess and comment on the prevalence of each chain. Some questions to consider (you don’t need to answer all of these and you may come up with your own):\n",
    "\n",
    "Are some of these chains more prevalent in certain states than others? Possibly despite having less stores overall? Same questions for regions instead of states.\n",
    "\n",
    "How does your chosen financial metric change by state and region for each chain? For example, having 5 stores in California is very different from having 5 stores in Wyoming.\n",
    "\n",
    "Does the distribution of each chain’s stores match population distribution, by both state/region?\n",
    "\n",
    "Do the financial data match what you’d expect based on the number and locations of the stores? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate\n",
    "Convert your code for Exercises 1-3 above to a function that takes a single argument: the URL. This function should\n",
    "\n",
    "Scrape the information on state names and corresponding number of store locations on the webpage specified (assume the page has a table in the same form and placement as the ones you scraped above)\n",
    "\n",
    "Extract the name of the company from either the URL specified or the webpage (assume the URL will have the same format as the ones used above)\n",
    "\n",
    "Return a clean, organized and tidy dataset. Find a page other than Starbucks and Dunkin’ Donuts to test this on to confirm that it works. It’s fine if this is not related to coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", class_ = \"wpr-table\")\n",
    "\n",
    "    rows = []\n",
    "    states_list = []\n",
    "\n",
    "    for th in table.find_all(\"th\")[4:]:\n",
    "        states = th.get_text(strip = True)\n",
    "        states_list.append(states)\n",
    "\n",
    "    for i, tr in enumerate(table.find_all(\"tr\")[1:]):\n",
    "        # iterate across states list to get corresponding state for each row\n",
    "        state = states_list[i]\n",
    "\n",
    "        # get all the rows in the dataset\n",
    "        cells = tr.find_all(\"td\")\n",
    "\n",
    "        # get all 2023 stores\n",
    "        stores2023_tag = cells[0].find(\"a\") or cells[0]\n",
    "        stores2023 = stores2023_tag.get_text(strip = True)\n",
    "        \n",
    "        #get all 2021 stores\n",
    "        stores2021_tag = cells[1].find(\"a\") or cells[1]\n",
    "        stores2021 = stores2021_tag.get_text(strip = True)\n",
    "\n",
    "        # get all 2024 stores\n",
    "        stores2024_tag = cells[2].find(\"a\") or cells[2]\n",
    "        stores2024 = stores2024_tag.get_text(strip = True)\n",
    "\n",
    "        # add columns to a list\n",
    "        rows.append({\n",
    "            \"state\" : state,\n",
    "            \"2023\" : stores2023,\n",
    "            \"2021\" : stores2021,\n",
    "            \"2024\" : stores2024\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# test function\n",
    "\n",
    "url = \"https://worldpopulationreview.com/state-rankings/chick-fil-a-by-state\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", class_ = \"wpr-table\")\n",
    "\n",
    "headers = []\n",
    "rows = []\n",
    "states_list = []\n",
    "\n",
    "# get state names and add to states list\n",
    "for th in table.find_all(\"th\", class_ = \"datatable-th\"):\n",
    "    header = th.get_text(strip = True)\n",
    "    headers.append(header)\n",
    "index = len(table.find_all(\"th\", class_ = \"datatable-th\"))\n",
    "\n",
    "for th in table.find_all(\"th\")[index:]:\n",
    "    states = th.get_text(strip = True)\n",
    "    states_list.append(states)\n",
    "\n",
    "\n",
    "for i, tr in enumerate(table.find_all(\"tr\")[1:]):\n",
    "    state = states_list[i]\n",
    "\n",
    "    cells = tr.find_all(\"td\")\n",
    "\n",
    "    stores2024_tag = cells[0].find(\"a\") or cells[0]\n",
    "    stores2024 = stores2024_tag.get_text(strip = True)\n",
    "\n",
    "    stores2023_tag = cells[1].find(\"a\") or cells[1]\n",
    "    stores2023 = stores2023_tag.get_text(strip = True)\n",
    "\n",
    "    rows.append({\n",
    "        \"state\" : state,\n",
    "        \"2023\" : stores2023,\n",
    "        \"2024\" : stores2024\n",
    "\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "Chatgpt uses\n",
    "- enumerate function\n",
    "- convert dictionary to a df\n",
    "\n",
    "\n",
    "\n",
    "Revenue data sources:\n",
    "\n",
    "Starbucks yearly revenue: https://companiesmarketcap.com/starbucks/revenue/\n",
    "\n",
    "Dunkin donuts yearly revenue: \n",
    "https://www.zippia.com/dunkin-donuts-careers-554008/revenue/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "raw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
